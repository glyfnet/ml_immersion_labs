{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Lab Setup\n",
    "\n",
    "## Setup & Admin\n",
    "This notebook is only used for setup and configuration of the environments. If your not runnning the labe, you shouldnt execute any of the code below!\n",
    "\n",
    "#### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configured settings\n"
     ]
    }
   ],
   "source": [
    "# user info \n",
    "N_USERS = 1\n",
    "\n",
    "# AWS account and s3 bucket \n",
    "# MODIFY  TO BE SPECIFIC FOR YOUR LAB\n",
    "REGION = 'ap-southeast-2'\n",
    "ACCOUNT= '013615763154'\n",
    "BUCKET = 'grr.amazon.com-lab' \n",
    "\n",
    "# security\n",
    "GROUP = 'labgroup'\n",
    "POLICY = 'arn:aws:iam::aws:policy/AdministratorAccess'\n",
    "NOTEBOOK_SERVICE_ROLE ='arn:aws:iam::013615763154:role/NotebookServiceRole'\n",
    "\n",
    "# notebook config\n",
    "VOLUME_SIZE=10\n",
    "INSTANCE_TYPE = 'ml.t3.large'\n",
    "'''\n",
    "Available Instance Types may vary by region:\n",
    "'ml.t2.medium'|'ml.t2.large'|'ml.t2.xlarge'|'ml.t2.2xlarge'|\n",
    "'ml.t3.medium'|'ml.t3.large'|'ml.t3.xlarge'|'ml.t3.2xlarge'|\n",
    "'ml.m4.xlarge'|'ml.m4.2xlarge'|'ml.m4.4xlarge'|'ml.m4.10xlarge'|'ml.m4.16xlarge'|\n",
    "'ml.m5.xlarge'|'ml.m5.2xlarge'|'ml.m5.4xlarge'|'ml.m5.12xlarge'|'ml.m5.24xlarge'|\n",
    "'ml.c4.xlarge'|'ml.c4.2xlarge'|'ml.c4.4xlarge'|'ml.c4.8xlarge'|\n",
    "'ml.c5.xlarge'|'ml.c5.2xlarge'|'ml.c5.4xlarge'|'ml.c5.9xlarge'|'ml.c5.18xlarge'|\n",
    "'ml.c5d.xlarge'|'ml.c5d.2xlarge'|'ml.c5d.4xlarge'|'ml.c5d.9xlarge'|'ml.c5d.18xlarge'|\n",
    "'ml.p2.xlarge'|'ml.p2.8xlarge'|'ml.p2.16xlarge'|\n",
    "'ml.p3.2xlarge'|'ml.p3.8xlarge'|'ml.p3.16xlarge'\n",
    "'''\n",
    "\n",
    "LABS_REQUIRED = ['deepar', 'groundtruth']\n",
    "\n",
    "# GROUND TRUTH & IMAGES\n",
    "#CODE_REPO = 'https://github.com/edenduthie/auto-labelling-model-tuning-amazon-sagemaker'\n",
    "#IMPORT_BUCKET = 'grr.amazon.com-public-share'\n",
    "#IMPORT_PATH = 'lab_deepar_data.zip'\n",
    "MAX_IMAGES = 1000\n",
    "\n",
    "print('configured settings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, boto3, random, string, time, json\n",
    "from zipfile import ZipFile\n",
    "from collections import defaultdict\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "sgm = boto3.client('sagemaker', REGION)\n",
    "iam = boto3.client('iam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate usernames and password\n",
    "count = 0\n",
    "colors = ['yellow', 'orange', 'red', 'green', 'blue', 'purple', 'silver', 'magenta', 'cyan', 'gray']\n",
    "things = ['fish', 'bird', 'bike', 'flower', 'house']\n",
    "usernames = []\n",
    "passwords = []\n",
    "\n",
    "def randomPassword(stringLength=8):\n",
    "    return ''.join(random.choice(string.ascii_lowercase) for i in range(stringLength))\n",
    "\n",
    "for color in colors:\n",
    "    for thing in things:\n",
    "        if count < N_USERS:\n",
    "            usernames.append(color + thing)\n",
    "            passwords.append(randomPassword())\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up individual labs\n",
    "Copy an archive of data from a public S3 bucket into this notbooks server and then upload to the labs shared s3 bucket in uncompressed format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unable to create bucket grr.amazon.com-lab\n",
      "downloading s3://grr.amazon.com-public-share/lab_deepar_data.zip\n",
      "wrote s3://grr.amazon.com-lab/labs/deepar/data/audusd_1m.csv\n",
      "wrote s3://grr.amazon.com-lab/labs/deepar/data/audusd_1m_partial.csv\n",
      "wrote s3://grr.amazon.com-lab/labs/deepar/data/eurusd_1m.csv\n",
      "wrote s3://grr.amazon.com-lab/labs/deepar/data/usdjpy_1m.csv\n"
     ]
    }
   ],
   "source": [
    "if 'deepar' in LABS_REQUIRED:\n",
    "    CODE_REPO = 'https://github.com/glyfnet/lab-sagemaker-deepar.git'\n",
    "    IMPORT_BUCKET = 'grr.amazon.com-public-share'\n",
    "    IMPORT_PATH = 'lab_deepar_data.zip'\n",
    "\n",
    "    try:\n",
    "        s3.Bucket(BUCKET).create(CreateBucketConfiguration={'LocationConstraint': REGION})\n",
    "        print('created bucket {}'.format(BUCKET))\n",
    "    except:\n",
    "        print('unable to create bucket '+BUCKET)\n",
    "\n",
    "    print('downloading s3://{}/{}'.format(IMPORT_BUCKET, IMPORT_PATH))\n",
    "    s3.Object(IMPORT_BUCKET, IMPORT_PATH).download_file(IMPORT_PATH)\n",
    "\n",
    "    with ZipFile(IMPORT_PATH, mode='r') as zip_ref:\n",
    "        for info in zip_ref.infolist():\n",
    "            with zip_ref.open(info) as file_ref:\n",
    "                if '__'  not in info.filename and not info.is_dir():\n",
    "                    key = 'labs/deepar/'+info.filename\n",
    "                    s3.Object(BUCKET, key).upload_fileobj(file_ref)\n",
    "                    print('wrote s3://{}/{}'.format(BUCKET, key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-05-18 03:40:31--  https://storage.googleapis.com/openimages/2018_04/test/test-annotations-bbox.csv\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.167.80, 2404:6800:4006:805::2010\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.167.80|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 52174204 (50M) [text/csv]\n",
      "Saving to: ‘test-annotations-bbox.csv.32’\n",
      "\n",
      "test-annotations-bb 100%[===================>]  49.76M  23.2MB/s    in 2.1s    \n",
      "\n",
      "2019-05-18 03:40:34 (23.2 MB/s) - ‘test-annotations-bbox.csv.32’ saved [52174204/52174204]\n",
      "\n",
      "--2019-05-18 03:40:34--  https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy.json\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.167.80, 2404:6800:4006:806::2010\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.167.80|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 86291 (84K) [text/csv]\n",
      "Saving to: ‘bbox_labels_600_hierarchy.json.32’\n",
      "\n",
      "bbox_labels_600_hie 100%[===================>]  84.27K  --.-KB/s    in 0.003s  \n",
      "\n",
      "2019-05-18 03:40:35 (28.8 MB/s) - ‘bbox_labels_600_hierarchy.json.32’ saved [86291/86291]\n",
      "\n",
      "Copying 1000/1000     \n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "if 'groundtruth' in LABS_REQUIRED:\n",
    "    # Download and process the Open Images annotations.\n",
    "    !wget https://storage.googleapis.com/openimages/2018_04/test/test-annotations-bbox.csv\n",
    "    !wget https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy.json\n",
    "    \n",
    "    with open('bbox_labels_600_hierarchy.json', 'r') as f:\n",
    "        hierarchy = json.load(f)\n",
    "    \n",
    "    CLASS_NAME = 'Bird'\n",
    "    CLASS_ID = '/m/015p6'\n",
    "\n",
    "    # Find all the subclasses of the desired image class (e.g. 'swans' and 'pigeons' etc if CLASS_NAME=='Bird').\n",
    "    good_subclasses = set()\n",
    "    def get_all_subclasses(hierarchy, good_subtree=False):\n",
    "        if hierarchy['LabelName'] == CLASS_ID:\n",
    "            good_subtree = True\n",
    "        if good_subtree:\n",
    "            good_subclasses.add(hierarchy['LabelName'])\n",
    "        if 'Subcategory' in hierarchy:            \n",
    "            for subcat in hierarchy['Subcategory']:\n",
    "                get_all_subclasses(subcat, good_subtree=good_subtree)\n",
    "        return good_subclasses\n",
    "    good_subclasses = get_all_subclasses(hierarchy)\n",
    "\n",
    "    # Find an appropriate number of images with at least one bounding box in the desired category\n",
    "    fids2bbs = defaultdict(list)\n",
    "    # Skip images with risky content.\n",
    "    skip_these_images = ['251d4c429f6f9c39', \n",
    "                        '065ad49f98157c8d']\n",
    "\n",
    "    with open('test-annotations-bbox.csv', 'r') as f:\n",
    "        for line in f.readlines()[1:]:\n",
    "            line = line.strip().split(',')\n",
    "            img_id, _, cls_id, conf, xmin, xmax, ymin, ymax, *_ = line\n",
    "            if img_id in skip_these_images:\n",
    "                continue\n",
    "            if cls_id in good_subclasses:\n",
    "                fids2bbs[img_id].append([CLASS_NAME, xmin, xmax, ymin, ymax])\n",
    "                if len(fids2bbs) == MAX_IMAGES:\n",
    "                    break\n",
    "\n",
    "    class CopyWorker(Thread):\n",
    "        def __init__(self, queue, src_bucket_name, dst_bucket_name):\n",
    "            self._queue = queue\n",
    "            self._src_bucket_name = src_bucket_name\n",
    "            self._dst_bucket = s3.Bucket(dst_bucket_name)\n",
    "            super(CopyWorker, self).__init__()\n",
    "\n",
    "        def run(self):\n",
    "            while True:\n",
    "                srckey, destkey = self._queue.get()\n",
    "                self._dst_bucket.copy(\n",
    "                    CopySource={'Bucket': self._src_bucket_name,'Key': srckey},\n",
    "                    Key= destkey,\n",
    "                )\n",
    "                self._queue.task_done()\n",
    "                sys.stdout.write('Copying {}/{}     \\r'.format(MAX_IMAGES-copy_queue.qsize(), MAX_IMAGES))\n",
    "                sys.stdout.flush() \n",
    "\n",
    "    # create a thread queue and start processing\n",
    "    copy_queue = Queue(maxsize=1000)\n",
    "    for thread in range(20):\n",
    "        worker = CopyWorker(copy_queue, 'open-images-dataset', BUCKET)\n",
    "        worker.daemon = True\n",
    "        worker.start()\n",
    "\n",
    "    # Copy the images to our local bucket.\n",
    "    for img_id_id, img_id in enumerate(fids2bbs.keys()):\n",
    "        srckey = 'test/{}.jpg'.format(img_id)\n",
    "        destkey = '{}/images/{}.jpg'.format('labs/groundtruth', img_id)   \n",
    "        copy_queue.put((srckey,destkey))\n",
    "\n",
    "    copy_queue.join()\n",
    "\n",
    "    print('\\nDone!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Create and upload the full input manifest.\n",
    "    def create_manifest(name, size=None):   \n",
    "        count = 0\n",
    "        with open(name, 'w') as f:\n",
    "            for img_id_id, img_id in enumerate(fids2bbs.keys()):\n",
    "                img_path = 's3://{}/{}/images/{}.jpg'.format(BUCKET, 'labs/groundtruth', img_id)\n",
    "                f.write('{\"source-ref\": \"' + img_path +'\"}\\n')\n",
    "                count += 1\n",
    "                if size is not None and count==size:\n",
    "                    break;\n",
    "                    \n",
    "        s3.Bucket(BUCKET).upload_file(name, 'labs/groundtruth' + '/' + name)\n",
    "    \n",
    "    create_manifest('input.manifest')\n",
    "    create_manifest('test.input.manifest', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create group labgroup with view only access to console\n",
    "try:\n",
    "    iam.create_group(GroupName=GROUP)\n",
    "    iam.attach_group_policy(GroupName=GROUP, PolicyArn=POLICY)\n",
    "    print('created group {} with policy {}'.format(GROUP, POLICY))\n",
    "except:\n",
    "    print('unable to create group '+ GROUP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create users and add to lab group\n",
    "count = 0\n",
    "for username, password in zip(usernames,passwords): \n",
    "    try:\n",
    "        iam.create_user(UserName=username)\n",
    "        iam.add_user_to_group(GroupName=GROUP, UserName=username)\n",
    "        iam.create_login_profile(UserName=username, Password=password)\n",
    "        print('created user '+username)\n",
    "        count += 1\n",
    "    except:\n",
    "        print('unable to create user ' + username)\n",
    "print('created {} users'.format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create notebooks\n",
    "count = 0\n",
    "for username in usernames:\n",
    "    try:\n",
    "        response = sgm.create_notebook_instance(\n",
    "            NotebookInstanceName=username,\n",
    "            InstanceType=INSTANCE_TYPE,\n",
    "            RoleArn=NOTEBOOK_SERVICE_ROLE,\n",
    "            DirectInternetAccess='Enabled',\n",
    "            VolumeSizeInGB=VOLUME_SIZE,\n",
    "            DefaultCodeRepository=CODE_REPO,\n",
    "        )\n",
    "        print('created notebook '+username) \n",
    "        count+=1\n",
    "    except:\n",
    "        print('unable to create notebook '+username) \n",
    "print('created {} notebooks'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create advice print out sheet\n",
    "for username, password in zip(usernames, passwords):\n",
    "    print('Login to console: https://console.aws.amazon.com ')\n",
    "    print('account: {}   user: {}   password: {}\\n'.format(ACCOUNT, username, password))\n",
    "    print('Open your notebook: Go to Amazon SageMaker -> Notebook Instances\\nOpen jupyter for {}\\n'.format(username))\n",
    "    print('Modify settings:')\n",
    "    print('BUCKET: {}   labid: {}   user: {}\\n\\n\\n'.format(BUCKET, LABID, username))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLEANUP ONLY!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop notebooks\n",
    "for username in usernames:   \n",
    "    try:\n",
    "        response = sgm.stop_notebook_instance(NotebookInstanceName=username)\n",
    "        print('stopped notebook '+username)\n",
    "    except: \n",
    "        print('unable to stop notebook '+username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all users & groups\n",
    "for username in usernames:\n",
    "    try:\n",
    "        response = iam.remove_user_from_group(GroupName=GROUP, UserName=username)\n",
    "        response = iam.delete_login_profile(UserName=username)\n",
    "        response = iam.delete_user(UserName=username)\n",
    "        print('removed user '+username)\n",
    "    except:\n",
    "        print('unable to remove user '+username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    iam.detach_group_policy(GroupName=GROUP, PolicyArn=POLICY)\n",
    "    iam.delete_group(GroupName=GROUP)\n",
    "    print('removed group '+GROUP)\n",
    "except:\n",
    "    print('unable to remove group '+GROUP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete notebooks\n",
    "for username in usernames:   \n",
    "    try:\n",
    "        response = sgm.delete_notebook_instance(NotebookInstanceName=username)\n",
    "        print('deleted notebook '+username)\n",
    "    except: \n",
    "        print('unable to delete notebook '+username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
