{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Currency Price & Volume with DeepAR\n",
    "## Part 1 - Feature Engineering\n",
    "\n",
    "This lab will walk you through the main steps of preparing a data set for training, testing and validation of a deep learning model called DeepAR that is available as an in-built SageMaker model. The data to be transformed is composed of one minute intervals of curreny price and volume data from 2107-2019.\n",
    "\n",
    "### Introduction\n",
    "\n",
    "#### DeepAR\n",
    "DeepAR is a deep learning model that leverages multiple layers of recurrent neural networks to predict time series quantities. The model takes as input a context of historical data, and generates quantiles of forecasted values. Deep AR is based on research done by Amazon to improve our own internal forecasting capabailities. To read more on how the algorithm works, [go here](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_how-it-works.html).\n",
    "\n",
    "> **What Are Quantiles?**\n",
    "> Quantiles percentages that represent the likelyhood a value will be lower or higher than a value. For example, a 50% quantile of 10, means that 50% of values will higher, and 50% lower than 10. This is also the mean. A 10% quantile of 10, means that 10% of values will be higher than 10, and 90% will be lower.\n",
    "\n",
    "#### Deep AR Features\n",
    "The model you will develop will forecast 30 minutes of volume and price quantiles for currencies, based on the previous 30 minutes of values. In order to do this, you need to generate features in a specific format that can be read by the Sage Maker model in training as well as inference. The format is composed of the following properties:\n",
    "\n",
    "1. Start Time - DeepAR will utilise seasonal and periodic correlations when tarining the model. It needs the start time as a feature in order to do this.\n",
    "2. Target Values - A list of time ordered values. For this lab the value will be one of open, close, high or low price or volume.\n",
    "3. Categorial Array - A list of categorical values to distinguish different time series from each other. For this lab, the categorical array will have two values, one for the currency type, and another for the type of value (open, close, high or low price or volume)\n",
    "\n",
    "A full row will look like this:\n",
    "```\n",
    "Row(\n",
    "    cat    : [currency_id, value_type_id],\n",
    "    start  : Timestamp of start of time series,\n",
    "    target : Array( values )\n",
    ")\n",
    "```   \n",
    "\n",
    "#### Spark\n",
    "Spark is used to process the data due to the size of the dataset. For the tutorial, you will only use a subset of data in the interest of time. You also only use a local (non-clustered) instance of Spark. For the full production model, you would need to process far more years or data as well as addtional currency pairs. A dataset that large will not fit into memory, and processing would be impossible on a single computer. This is a common problem in machine learning that requires large scale data sets. Spark solves these issues by providing a way to process data using horizontally scalable compute instances. It also provides an SQL langauge that makes it easy to use for data scienctists with development or analyst backgrounds.\n",
    "\n",
    "### 1. Get Started\n",
    "To begin, modify the cell below so your generated s3 data will not interfere with other people working on the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER = <YOUR USER NAME IN QUOTES> # example 'whitefish'\n",
    "BUCKET = <LAB BUCKET NAME in QUOTES>  # example 'grr.amazon.com-lab'\n",
    "\n",
    "OVERIDE_PATH = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Configuration\n",
    "First, you will import all the required python and Spark libraries and create a spark session. You need to include the required SageMaker classpath which enables access to s3 from within Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import sagemaker_pyspark, boto3\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, DoubleType, StringType, TimestampType, StructType, StructField\n",
    "import json\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n",
    "spark = SparkSession.builder.config(\"spark.driver.extraClassPath\", classpath).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read CSV File\n",
    "Now you can create a Spark data frame from the csv file that contains Australian dollar versus US Dollar. The file is located in s3. After loading the data into a data frame, you create a temporary view that allows us to query the data with Spark SQL queries. You also rename one of the columns so it is easier to deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath = 's3a://{}/labs/deepar/data/audusd_1m_partial.csv'.format(BUCKET)\n",
    "print('reading csv file {}'.format(inputpath))\n",
    "df = spark.read.csv(inputpath, inferSchema=True, header=True).withColumnRenamed('Gmt time', 'Time')\n",
    "df.createOrReplaceTempView('forex')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Standardize & Order Data\n",
    "You need our current string-based date column to be a timestamp type as you will use this column for ordering and creating subsets of data by time. Once you have the column cast to a timestamp, you can order the values in ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT \n",
    "    to_timestamp(Time, \"dd.MM.yyyy HH:mm:ss.SSS\") as start, \n",
    "    *\n",
    "FROM forex \n",
    "ORDER BY start\n",
    "'''\n",
    "spark.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create Weekly Time Series\n",
    "As opposed to treating the entire 2 years of time series as a single time series, you will group them up into weeks. To do this you create a group column that is comnposed of the year and week of year.\n",
    "\n",
    "#### Create Group Column\n",
    "You will append the year and week of year into a unique group identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT \n",
    "    weekofyear(start) + year(start) * 100 as group,\n",
    "    *\n",
    "FROM (\n",
    "    SELECT \n",
    "        to_timestamp(Time, \"dd.MM.yyyy HH:mm:ss.SSS\") as start, \n",
    "        * \n",
    "    FROM forex \n",
    ")\n",
    "'''  \n",
    "spark.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group Into Lists\n",
    "Once you have the group column, you can group the entire data set by group, collecting each column into a list of values for each group. You also get the start date for the time series by finding the minimum timestamp within a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT\n",
    "    min(start) as ts_start,\n",
    "    collect_list(Open) as open,\n",
    "    collect_list(Close) as close,\n",
    "    collect_list(Low) as low,\n",
    "    collect_list(Volume) as volume\n",
    "FROM (\n",
    "    SELECT \n",
    "        weekofyear(start) + year(start) * 100 as group,\n",
    "        *\n",
    "    FROM (\n",
    "        SELECT \n",
    "            to_timestamp(Time, \"dd.MM.yyyy HH:mm:ss.SSS\") as start, \n",
    "            *        \n",
    "        FROM forex \n",
    "    ) ORDER BY start\n",
    ") GROUP BY group ORDER BY group\n",
    "'''\n",
    "df = spark.sql(query)\n",
    "df.createOrReplaceTempView('features')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create Training Set\n",
    "Now that you have the data in the right format, you need to split of the training set. For time series data, the training set should have no visibility into the time period of the test set, to insure the model is not just fitting to the known time period. \n",
    "\n",
    "#### Get Cut-offs Dates\n",
    "First, you need to get the start date based on the minimum of all the starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT \n",
    "    min(size(volume)) AS min_cnt, \n",
    "    max(size(volume)) AS max_cnt,\n",
    "    min(ts_start) AS min_date, \n",
    "    max(ts_start) AS max_date \n",
    "FROM features\n",
    "'''\n",
    "\n",
    "stats = spark.sql(query).collect()[0]\n",
    "spark.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three sets of data. The training set is for training the model. The test set is for calculating the performance of the model during hyperparameter tuning and training evaluation. The holdout set is used for the final evaluation of the model to see how it will perform on new data.\n",
    "\n",
    "To better understand how the time is divided up, you will print a diagram of the different cut-off times for each of these sets. You use the weeks of holdout, and weeks of test to calculate these time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('time series have between {} and {} entries\\n\\n'.format(stats.min_cnt, stats.max_cnt))\n",
    "\n",
    "holdout_start = stats.max_date - timedelta(days=(7*6)) # 8 weeks of holdout\n",
    "test_start = holdout_start - timedelta(days=8*7)      # 8 weeks of test set\n",
    "holdout_end = stats.max_date\n",
    "\n",
    "print('    |....... train .......|....... test .......|....... holdout .......|')\n",
    "print('{}            {}           {}              {}'.format(\n",
    "    stats.min_date.date(), \n",
    "    test_start.date(), \n",
    "    holdout_start.date(), \n",
    "    holdout_end.date())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store To S3\n",
    "Now that you have the cut-off dates, you can select the features and save them to an S3 bucket of training data. For training, you write the data in compressed parquet format. You also will add a category column `cat` with the first value being 0 for currency `audusd` and the second value being 0 for the `close`. Eventually when you wish to infer forecasts with the trained model, you must pass in same the category id for the currency and value types.\n",
    "\n",
    "> **Note** For the tutorial, you limit the query results so the write operation does not take too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT \n",
    "    date_format(ts_start, 'yyyy-MM-dd HH:mm:ss') as start, \n",
    "    close as target,\n",
    "    array(0,0) as cat\n",
    "FROM features\n",
    "WHERE ts_start < to_timestamp('2018-10-16', 'yyyy-MM-dd')\n",
    "LIMIT 100\n",
    "'''\n",
    "train = spark.sql(query)\n",
    "\n",
    "outpath = 's3a://{}/labs/deepar/output/{}/dev_features/train'.format(BUCKET, USER)\n",
    "train.write.mode(\"append\").parquet(outpath)\n",
    "\n",
    "print('wrote train data to '+outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Create Test Set\n",
    "Now you create the test from data beginning at the end of the training set range, and ending at the beginning of the holdout set. For training data, you need the time series to begin at different times, so you create a function to randomly generate different time ranges. The time ranges need to be the same as the model requires for input, and also have the same output length. For the example, you create 5 random subsets from each timeseries, and make them 60 minutes in length, since the model requires 30 minutes of data and forecast 30 minutes into the future. None of this is needed for training data, as the training algorithm will do this automatically.\n",
    "\n",
    "#### Create UDF\n",
    "The code below defines a Spark user defined function (UDF) that you can use to generate the sub samples required for the test set, all within a Spark SQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def create_tests(start, values, length, ntests):\n",
    "    \n",
    "    # create random set of start indexes\n",
    "    start_indexes = random.sample(range(0, len(values) - length), ntests)\n",
    "    \n",
    "    # create new sequences for each start index of the required length\n",
    "    tests = []\n",
    "    for s in start_indexes:\n",
    "        tests.append({\n",
    "            'ts_start': start + timedelta(minutes=s),\n",
    "            'target': values[s: s+length]\n",
    "        })   \n",
    "    return tests\n",
    "\n",
    "# define the return type\n",
    "ret_type =  ArrayType(\n",
    "                 StructType([\n",
    "                    StructField('ts_start', TimestampType(), True),\n",
    "                    StructField('target', ArrayType(DoubleType()), True)\n",
    "                ])\n",
    "            )\n",
    "\n",
    "# register the udf\n",
    "spark.udf.register(\"create_tests\", create_tests, ret_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Test Data\n",
    "Now that you have the UDF you can query the test data with subsamples. The UDF generates lists or time series in a single row. Sage Maker requires a single time series per row, so you need to explode each row into multiple rows using the 'LATERAL VIEW explode' syntax below. You also order the rows randomly, so when Sage Maker evaluates the model it looks at different types of series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT \n",
    "    date_format(test.ts_start, 'yyyy-MM-dd HH:mm:ss') as start,\n",
    "    test.target AS target,\n",
    "    array(0,0) as cat\n",
    "FROM (\n",
    "    SELECT create_tests(ts_start, close, 60, 5) as targets \n",
    "    FROM features \n",
    "    WHERE ts_start >= to_timestamp('2018-10-16', 'yyyy-MM-dd')  AND ts_start < to_timestamp('2018-11-27', 'yyyy-MM-dd')\n",
    "    LIMIT 100\n",
    ")\n",
    "LATERAL VIEW explode(targets) t as test \n",
    "ORDER BY rand()\n",
    "'''\n",
    "test = spark.sql(query)\n",
    "test.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the Test Data\n",
    "Now you can save the test data to an s3 bucket. The exact same process can be followed for creating a holdout dataset, using the holdout set time cut-offs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = 's3a://{}/labs/deepar/output/{}/dev_features/test'.format(BUCKET, USER)\n",
    "test.write.parquet(outpath, mode='append')\n",
    "print('wrote test data to '+outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. ETL with Glue\n",
    "All the above code can now be gathered into a single Glue script and executed across many different currencies, as well as different data columns like volume, high price, or low price. Glue enables us to do serverless Spark jobs and avoid setting up an EMR cluster for our spark jobs. Up until now, you have been running Spark locally on a small set of data. When you start adding in more years of data, more currencies, and more test and holdout set samples, this would quickly become untenable to run on a single node. \n",
    "\n",
    "Some key difference in the glue script to the above code is that is parameterized, so you can generate different types of data sets. It enables multiple currencies and currency properties. Each time series is categorized by the instrument and property type, so the model can distinguish them from each other. In addition, it generates a holdout set in the same way the test set was generated. The holdout set is generated in json, to enable batch scoring.\n",
    "\n",
    "The Glue script will also randomly shuffle all the stored data, so the training batches don't overly fit to one type of time series.\n",
    "\n",
    "#### Create Glue Job\n",
    "The following script creates the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, time\n",
    "\n",
    "# get the s3 and glue client\n",
    "glue = boto3.client('glue')\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# save the glue script to s3\n",
    "filepath = 'labs/deepar/output/{}/scripts/glue_create_features.py'.format(USER)\n",
    "s3.Object(BUCKET, filepath).upload_file('glue_create_features.py')\n",
    "script_path = 's3://{}/{}'.format(BUCKET, filepath)\n",
    "print('wrote glue script to {}'.format(script_path))\n",
    "\n",
    "# set the glue log directory\n",
    "log_dir = 's3://{}/labs/deepar/output/{}/glue_logs'.format(BUCKET, USER)\n",
    "\n",
    "# set the glue temp directory\n",
    "temp_dir = 's3://{}/labs/deepar/output/{}/glue_temp'.format(BUCKET, USER)\n",
    "\n",
    "# set the output path \n",
    "# the overide path is used to create the master data set for rest of lab\n",
    "if OVERIDE_PATH:\n",
    "    s3_path = overide_path\n",
    "else:\n",
    "    s3_path = 's3://{}/labs/deepar/output/{}/features'.format(BUCKET, USER)\n",
    "print('s3_path feature path is ' + s3_path)\n",
    "\n",
    "# create a glue job\n",
    "job_name = USER + time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "print('using job name '+job_name)\n",
    "\n",
    "glue.delete_job(JobName=job_name)\n",
    "response = glue.create_job(\n",
    "    Name=   job_name,\n",
    "    LogUri= log_dir,\n",
    "    Role='GlueServiceRole',\n",
    "    ExecutionProperty={ \n",
    "        'MaxConcurrentRuns': 1\n",
    "    },\n",
    "    Command={\n",
    "        'Name': 'glueetl',\n",
    "        'ScriptLocation': script_path\n",
    "    },\n",
    "    DefaultArguments={\n",
    "        '--TempDir': temp_dir,\n",
    "        '--enable-metrics': ''\n",
    "    },\n",
    "    MaxRetries=0,\n",
    "    AllocatedCapacity=60,\n",
    "    Timeout=600\n",
    ")\n",
    "print(\"\\ncreated job\")\n",
    "print(json.dumps(response, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Glue Job\n",
    "This script runs the job, by passing in parameters that define how the dataset will be generated. You can now check the glue console to see the job running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute the job\n",
    "response = glue.start_job_run(\n",
    "    JobName = job_name, \n",
    "        Arguments = {\n",
    "            '--instruments':   'audusd,eurusd,usdjpy',\n",
    "            '--columns'    :   'open,close,high,low',\n",
    "            '--test_weeks' :   '12',\n",
    "            '--n_tests':       '300',\n",
    "            '--holdout_weeks': '12',\n",
    "            '--length':        '60',\n",
    "            '--s3_bucket':    BUCKET,\n",
    "            '--s3_path':      s3_path,\n",
    "            '--user':         USER\n",
    "        }\n",
    ")\n",
    "print('\\nstarted job')\n",
    "print(json.dumps(response, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**\n",
    "To monitor the executing glue job, you can open the AWS console and goto `Services -> AWS Glue -> ETL / Jobs`\n",
    "\n",
    "\n",
    "### Stop Glue Job\n",
    "Since the job is a long running task, you wlll now stop it. The next lab will work off of a previously generated full feature set, so running the full feature generation job is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OVERIDE_PATH is None:\n",
    "    glue.batch_stop_job_run(JobName=job_name, JobRunIds=[response['JobRunId']])\n",
    "    print('stopped glue job '+response['JobRunId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glue Script\n",
    "The actual glue script can be viewed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat glue_create_features.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In this lab you have performed all the steps needed to prepare the data for training a Deep AR model. For your own data sets, the process will be very similar. For the final production system, some additional steps would be:\n",
    "1. Automation of Glue scripts with a schedule of processing times.\n",
    "2. Versioning of generated features to enable different model architectures.\n",
    "3. Augmenting the dataset with more currencies, or a longer time period.\n",
    "\n",
    "In the next tutorial, you will use the generated data to [tune and train the DeepAR model](Deep%20AR%20Tune%20%26%20Train.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
