{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Currency Price & Volume with DeepAR\n",
    "## Part 2 - Train and Evaluate the Model\n",
    "\n",
    "This lab will walk you through the main steps of tuning the hyperparameters required to train a Deep AR model. You will then then do a full training run of the Deep AR model using the discovered parameters. The goal is to create a single model that predicts multiple quantities 30 minutes into the future based on the previous 30 minutes of data. The values to be predicted include volume, high price, low price, open price and close price for the Australia dollar, Euro and Yen versus the US dollar.\n",
    "\n",
    "To begin, modify the cell below so your generated s3 data will not interfere with other people working on the lab. Use the same user name and lab date from the previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER = <YOUR USER NAME IN QUOTES> # example 'whitefish'\n",
    "BUCKET = <LAB BUCKET NAME in QUOTES>  # example 'grr.amazon.com-lab'\n",
    "\n",
    "# override these if did not complete previous labs data prep\n",
    "DATA_BUCKET = 'grr.amazon.com-lab' \n",
    "DATA_USER = 'example'\n",
    "EPOCHS = 1\n",
    "\n",
    "BEST_TRAINING_JOB = 'example-da-20190519030249'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Configuration\n",
    "First you need to import all the required python and SageMaker libraries and create a SageMaker session object. SageMaker uses the session to hold the local configuration state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, boto3, time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.tuner import HyperparameterTuner, ContinuousParameter, IntegerParameter\n",
    "\n",
    "# get sagemaker session, image uri for the deep ar model and the execution role\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Estimator\n",
    "Now you can create an estimator. The estimator is the main entry point object upon which hyperparameter tuning, training and batch transform jobs can be created and run.\n",
    "\n",
    "To configure the estimator, you need several things:\n",
    "\n",
    "1. An IAM execution role that grants the estimator permissions to exeute Sage Maker functions.\n",
    "2. The training instance type. Select a P2 instance.\n",
    "3. The number of training instances.\n",
    "4. The image name of the SageMaker model type. This could also be a docker image that holds your own model.\n",
    "5. The job name for identification.\n",
    "6. The s3 output path to store the model.\n",
    "7. The Sage Maker session previously created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = get_image_uri(boto3.Session().region_name, 'forecasting-deepar')\n",
    "role = get_execution_role()\n",
    "\n",
    "job_name = USER + '-da-' + time.strftime('%Y%m%d%H%M%S', time.gmtime())\n",
    "print('using job name '+job_name)\n",
    "\n",
    "output_path = 's3://{}/labs/deepar/output/{}/models'.format(BUCKET, USER)\n",
    "print ('creating deep ar models at '+output_path)\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.p2.xlarge',\n",
    "    base_job_name=job_name,\n",
    "    output_path=output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Data Channels\n",
    "You also need to set the data channels for training and testing. Data in test, train and holdout locations was generated in the previous tutorial. The buckets contain data in the format expected by deep ar. Since the previous tutorial didn’t complete due to time constraints, you will use a larger set previously generated of seven years of data for all three currencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_channels = {\n",
    "    'train': 's3://{}/labs/deepar/output/{}/features/train'.format(DATA_BUCKET, DATA_USER),\n",
    "    'test': 's3://{}/labs/deepar/output/{}/features/test'.format(DATA_BUCKET, DATA_USER),\n",
    "}\n",
    "print(data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Tuning Job\n",
    "For a hyperparameter tuning job, you can choose which parameters are fixed, and which ones you wish the tuner to vary. The documentation for Deep AR recommends the quantities below, so you will use those ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set static values\n",
    "estimator.set_hyperparameters(    \n",
    "    time_freq = '1min',\n",
    "    dropout_rate = 0.3,\n",
    "    epochs = 600,\n",
    "    prediction_length = '30',\n",
    "    context_length = '30',\n",
    "    likelihood = 'gaussian',\n",
    "    early_stopping_patience = '10',\n",
    "    mini_batch_size = 950,\n",
    "    test_quantiles = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    ")\n",
    "\n",
    "# Configure hyperparameter ranges\n",
    "hyp_tuner = HyperparameterTuner(\n",
    "        estimator=estimator,  \n",
    "        objective_metric_name='test:mean_wQuantileLoss',\n",
    "        objective_type='Minimize',\n",
    "        hyperparameter_ranges={\n",
    "            'learning_rate': ContinuousParameter(0.0005, 0.01),\n",
    "            'embedding_dimension': IntegerParameter(1, 10),\n",
    "            'num_cells' : IntegerParameter(100,200),\n",
    "            'num_layers': IntegerParameter(1,4)\n",
    "        },\n",
    "        max_jobs=50,\n",
    "        max_parallel_jobs=1,\n",
    "        early_stopping_type= 'Auto'\n",
    "    )\n",
    "\n",
    "# Run the tuning job\n",
    "hyp_tuner.fit(data_channels, job_name=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitor Tuning\n",
    "You can easily monitor the tuning job through API calls, or through the AWS console. The function below prints out basic details about the ongoing tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import monitor_tuner\n",
    "monitor_tuner(job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**\n",
    "To monitor the executing job, you can also open the AWS console and go to `SageMaker -> Hyperparameter Tunning`\n",
    "\n",
    "#### Stop Tuning Job\n",
    "Since the tuning job is a long running task (over 20 hours), you will now stop it. The next steps in the lab will use the values from a job that was run previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_tuner.stop_tuning_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "For re-training a model on new data, you can skip hyperparameter tuning, and use the same hyperparameters values. When the data changes significantly, you can always re-run a hyperparameter tuning job, with a tighter range of values around what worked best last time. The steps to run a training job are very similar to the hyperparameter job, the difference being you specify static parameters as opposed to ranges. Execute the code below to start a training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(    \n",
    "        time_freq = '1min',\n",
    "        context_length = '30',\n",
    "        prediction_length = '30',\n",
    "        num_cells = '132',\n",
    "        num_layers = '1',\n",
    "        likelihood = 'gaussian',\n",
    "        epochs = EPOCHS,\n",
    "        mini_batch_size = '600',\n",
    "        learning_rate = '0.008',\n",
    "        dropout_rate = '0.3',\n",
    "        early_stopping_patience = '20',\n",
    "        embedding_dimension = '1',\n",
    "        test_quantiles = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "    )\n",
    "\n",
    "estimator.fit(data_channels, wait=False, job_name=job_name)\n",
    "print('training '+job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Training Job\n",
    "Since the training job is a long running task (over 1 hour), you will now stop it. The training call is synchronous, so you will need to hit the stop button on the notebook before preceding. This does not stop the server-side training job. So be sure to execute the stop call below. The next steps in the lab will use a model that was trained previously, so we don’t need to wait for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.client('sagemaker', region_name=region).stop_training_job(TrainingJobName=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Endpoint\n",
    "Now that you have a trained model, you can create an endpoint that can be used for scoring in real time. To create the endpoint, you need the job name of the best training run. We will use a previously trained model for this exercise.\n",
    "\n",
    "> **Note** If only batch scoring is needed, you should not create an endpoint. An endpoint is always running and will be billed according to the on demand instance type it is deployed on. Batch jobs are also billed by instance type, but only for the duration of the batch job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach to best model from previous HP tuning job\n",
    "estimator = Estimator.attach(BEST_TRAINING_JOB)\n",
    "\n",
    "# example endpoint already deployed, skip this\n",
    "#predictor = estimator.deploy(initial_instance_count = 1, instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model\n",
    "\n",
    "The best way to evaluate the model perfromance is to graph actuals versus predicted over a wide range of varying time series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import helper\n",
    "from helper import predict_from_file, graph_predictions\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# find a non-empty holdout data file\n",
    "path = 'labs/deepar/output/{}/features/holdout/'.format(DATA_USER)\n",
    "response = s3.list_objects_v2(Bucket=DATA_BUCKET, MaxKeys=100, Prefix=path) \n",
    "for content in response['Contents']:\n",
    "    key = content['Key'].split('/')[-1]\n",
    "    if key.split('.')[-1] == 'json' and int(content['Size']) > 100:\n",
    "        break\n",
    "\n",
    "holdout_file = key\n",
    "\n",
    "# make some predictions\n",
    "requests, actuals, predicteds = predict_from_file(sagemaker_session, BEST_TRAINING_JOB, DATA_BUCKET, path, holdout_file, 30, 30, quantiles=[\"0.3\", \"0.4\", \"0.5\", \"0.6\", \"0.7\"])\n",
    "graph_predictions(requests, actuals, predicteds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete Endpoint\n",
    "Since you will be using batch scoring, you will now delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.client('sagemaker', region_name=region_name).delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Score\n",
    "In may be more effective and economical to use batch scoring on a large set of files. The code below demonstrates how to use the estimator to create a transform job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_dir = 's3://{}/labs/deepar/output/{}/features/holdout'.format(DATA_BUCKET, DATA_USER)\n",
    "predictions_dir = 's3://{}/labs/deepar/output/{}/predictions'.format(BUCKET, USER)\n",
    "print ('transforming {} to {}'.format(holdout_dir, predictions_dir))\n",
    "\n",
    "# create transformer\n",
    "transformer = estimator.transformer(\n",
    "        instance_count=1,\n",
    "        instance_type='ml.c4.xlarge',\n",
    "        output_path=predictions_dir,\n",
    "        env = {'output_types': 'quantiles'}\n",
    ")\n",
    "\n",
    "# transform holdout set to predictions\n",
    "transformer.transform(holdout_dir, split_type='Line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Transform Job\n",
    "Since the transform job is a long running task (over 1 hour), you will now stop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.client('sagemaker', region_name=region_name).stop_transform_job(TransformJobName=transformer._current_job_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
