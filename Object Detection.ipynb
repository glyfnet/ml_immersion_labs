{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an object detection model using Ground Truth labels\n",
    "At this stage, you have fully labeled your dataset and you can train a machine learning model to perform object detection. You'll do so using the **augmented manifest** output of your labeling job - no additional file translation or manipulation required! For a more complete description of the augmented manifest, see our other [example notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/ground_truth_labeling_jobs/object_detection_augmented_manifest_training/object_detection_augmented_manifest_training.ipynb).\n",
    "\n",
    "**NOTE:** Object detection is a complex task, and training neural networks to high accuracy requires large datasets and careful hyperparameter tuning. The following cells illustrate how to train a neural network using a Ground Truth output augmented manifest, and how to interpret the results. However, you shouldn't expect a network trained on 10 or 1000 images to do a great job on unseen images!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "Be sure to modify the below with your user name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER = <YOUR USER NAME IN QUOTES> # example 'whitefish'\n",
    "BUCKET = <LAB BUCKET NAME in QUOTES>  # example 'grr.amazon.com-lab'\n",
    "\n",
    "# these settings let you use example data in case your previous lab was not complete\n",
    "DATA_BUCKET = BUCKET # 'grr.amazon.com-lab' \n",
    "DATA_USER = 'example'\n",
    "S3_BASE_OVERRIDE = None #'s3://grr.amazon.com-lab/labs/groundtruth/output/all-bird-labels'\n",
    "EPOCHS = 1\n",
    "MINIBATCH = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import the required libs. You use the `%` magic directive to enable inline plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json, boto3, sagemaker, re, time, sys, glob, os\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker import get_execution_role\n",
    "import numpy as np\n",
    "from time import gmtime, strftime\n",
    "import importlib, helper\n",
    "importlib.reload(helper)\n",
    "from helper import training_status, visualize_detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "First, you willl split your augmented manifest into a training set and a validation set using an 80/20 split and save the results to files that the model will use during training. To do this, the output manifest is read to get a list of all the images that are labeled. Then "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if S3_BASE_OVERRIDE:\n",
    "    S3_BASE = S3_BASE_OVERRIDE  \n",
    "else:\n",
    "    S3_BASE =  's3://{}/labs/groundtruth/output/{}'.format(DATA_BUCKET, DATA_USER)\n",
    "    \n",
    "print(S3_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_manifest = S3_BASE + '/manifests/output/output.manifest'\n",
    "!aws s3 cp {output_manifest} temp/output.manifest\n",
    "with open('temp/output.manifest', 'r') as f:\n",
    "    output = [json.loads(line.strip()) for line in f.readlines()]\n",
    "print('read output manifest '+output_manifest)\n",
    "\n",
    "# Retrieve the worker annotations.\n",
    "worker_reponse = S3_BASE + '/annotations/worker-response'\n",
    "!aws s3 cp {worker_reponse} temp/od_output_data/worker-response --recursive --quiet\n",
    "\n",
    "# Find the worker files.\n",
    "worker_file_names = glob.glob(\n",
    "    'temp/od_output_data/worker-response/**/*.json', recursive=True)\n",
    "with open('temp/output.manifest', 'r') as f:\n",
    "    output = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "# Shuffle output in place.\n",
    "np.random.shuffle(output)\n",
    "    \n",
    "dataset_size = len(output)\n",
    "train_test_split_index = round(dataset_size*0.8)\n",
    "\n",
    "train_data = output[:train_test_split_index]\n",
    "validation_data = output[train_test_split_index:]\n",
    "\n",
    "num_training_samples = 0\n",
    "with open('temp/train.manifest', 'w') as f:\n",
    "    for line in train_data:\n",
    "        f.write(json.dumps(line))\n",
    "        f.write('\\n')\n",
    "        num_training_samples += 1\n",
    "print('created training manifest')\n",
    "    \n",
    "with open('temp/validation.manifest', 'w') as f:\n",
    "    for line in validation_data:\n",
    "        f.write(json.dumps(line))\n",
    "        f.write('\\n')\n",
    "print('created training manifest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll upload these manifest files to the previously defined S3 bucket so that they can be used in the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s3_prefix = 'labs/objectdetect/{}'.format(USER)\n",
    "s3_base_path = 's3://{}/{}'.format(BUCKET, s3_prefix)\n",
    "\n",
    "s3_output_path = s3_base_path + '/output'\n",
    "s3_train_data_path = s3_base_path + '/train.manifest'\n",
    "s3_validation_data_path = s3_base_path + '/validation.manifest'\n",
    "\n",
    "!aws s3 cp temp/train.manifest {s3_train_data_path}\n",
    "!aws s3 cp temp/validation.manifest {s3_validation_data_path}\n",
    "print('uploaded manifests to s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Training\n",
    "Now that you are done with all the setup that is needed, you are ready to train your object detector. To begin, create a sageMaker.estimator.Estimator object. This estimator will launch the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get required objects and parameters\n",
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "training_image = get_image_uri(region, 'object-detection', repo_version='latest')\n",
    "session = sagemaker.Session()\n",
    "\n",
    "# create an estimator\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    training_image,\n",
    "    role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.p2.xlarge',\n",
    "    train_volume_size=50,\n",
    "    train_max_run=360000,\n",
    "    input_mode='Pipe',\n",
    "    output_path=s3_output_path,\n",
    "    sagemaker_session = session\n",
    ")\n",
    "print('created estimator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object detection algorithm at its core is the Single-Shot Multi-Box detection algorithm (SSD). This algorithm uses a base_network, which is typically a VGG or a ResNet. The Amazon SageMaker object detection algorithm supports VGG-16 and ResNet-50 now. It also has a lot of options for hyperparameters that help configure the training job. The next step in your training, is to setup these hyperparameters and data channels for training the model. Consider the following example definition of hyperparameters. See the SageMaker Object Detection documentation for more details on the hyperparameters.\n",
    "\n",
    "One of the hyperparameters here for instance is the epochs. This defines how many passes of the dataset you iterate over and determines that training time of the algorithm. In this example, you train the model for 5 epochs to generate a basic model for the PASCAL VOC dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB. These hyperparameters are at the user's discretion and are beyond the scope of this demo.\n",
    "hyperparameters = {  \n",
    "            \"base_network\": \"resnet-50\",\n",
    "            \"use_pretrained_model\": \"1\",\n",
    "            \"num_classes\": \"1\",\n",
    "            \"mini_batch_size\": MINIBATCH,\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"learning_rate\": \"0.001\",\n",
    "            \"lr_scheduler_step\": \"\",\n",
    "            \"lr_scheduler_factor\": \"0.1\",\n",
    "            \"optimizer\": \"sgd\",\n",
    "            \"momentum\": \"0.9\",\n",
    "            \"weight_decay\": \"0.0005\",\n",
    "            \"overlap_threshold\": \"0.5\",\n",
    "            \"nms_threshold\": \"0.45\",\n",
    "            \"image_shape\": \"300\",\n",
    "            \"label_width\": \"350\",\n",
    "            \"num_training_samples\": str(num_training_samples)\n",
    "        }\n",
    "\n",
    "estimator.set_hyperparameters(**hyperparameters)\n",
    "print('set hyperparameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the hyperparameters are setup, prepare the handshake between your data channels and the algorithm. To do this, you need to create the sagemaker.session.s3_input objects from your data channels. These objects are then put in a simple dictionary, which the algorithm consumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_input(uri):\n",
    "    return sagemaker.session.s3_input(\n",
    "        uri, \n",
    "        s3_data_type='AugmentedManifestFile',\n",
    "        distribution='FullyReplicated', \n",
    "        content_type='application/x-recordio',\n",
    "        record_wrapping='RecordIO',\n",
    "        attribute_names = ['source-ref', DATA_USER]\n",
    "    )\n",
    "\n",
    "data_channels = {\n",
    "    'train': create_data_input(s3_train_data_path), \n",
    "    'validation': create_data_input(s3_validation_data_path)\n",
    "}\n",
    "print('created data channels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the Estimator object, you have to set the hyperparameters for the Estimator and  have the data channels linked with the algorithm. The only remaining thing to do is to train the algorithm. The following command will train the algorithm. Training the algorithm involves a few steps. Firstly, the instances that you requested while creating the Estimator classes are provisioned and are setup with the appropriate libraries. Then, the data from your channels are downloaded into the instance. Once this is done, the training job begins. The provisioning and data downloading will take time, depending on the size of the data. Therefore it might be a few minutes before you start getting data logs for your training jobs. The data logs will also print out Mean Average Precision (mAP) on the validation data, among other losses, for every run of the dataset once or one epoch. This metric is a proxy for the quality of the algorithm.\n",
    "\n",
    "Once the job has finished a \"Training job completed\" message will be printed. The trained model can be found in the S3 bucket that was setup as output_path in the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create unique job name\n",
    "job_name = USER + '-od-' + time.strftime('%Y%m%d%H%M%S', time.gmtime())\n",
    "print('using job name '+job_name)\n",
    "# start training\n",
    "estimator.fit(inputs=data_channels, wait=False, logs=True, job_name=job_name)\n",
    "print('training job launched')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the progess of the training job, you can repeatedly evaluate the following cell. When the training job status reads `'Completed'`, move on to the next part of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_status(job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hosting\n",
    "\n",
    "Once the training is done, you can deploy the trained model as an Amazon SageMaker real-time hosted endpoint. This will allow you to make predictions (or inference) from the model. Note that you don't have to host on the same instance (or type of instance) that you used to train. Training is a prolonged and compute heavy job that require a different of compute and memory requirements that hosting typically do not. You can choose any type of instance you want to host the model. In your case you chose the `ml.p3.2xlarge` instance to train, but you choose to host the model on the less expensive cpu instance, `ml.m4.xlarge`. The endpoint deployment can be accomplished as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_detector = estimator.deploy(initial_instance_count = 1, instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Now that the trained model is deployed at an endpoint that is up-and-running, you can use this endpoint for inference. To do this, you will download the previously created validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = []\n",
    "gt_bboxes = []\n",
    "s3 = boto3.resource('s3')\n",
    "for file_data in validation_data:\n",
    "    file_path = file_data['source-ref']\n",
    "    key = file_path.split(BUCKET+'/')[-1] # get the key portion of uri  \n",
    "    dest = 'od_output_data/'+file_path.split('/')[-1]\n",
    "\n",
    "    file_names.append(dest)\n",
    "    s3.Bucket(BUCKET).download_file(key, dest) # download file\n",
    "    \n",
    "    if 'annotations' in file_data[DATA_USER]:\n",
    "        annotations = file_data[DATA_USER]['annotations']\n",
    "        bboxes = []\n",
    "        for a in annotations:\n",
    "            bboxes.append([int(a['left']), int(a['top']), int(a['width']), int(a['height'])]) \n",
    "        gt_bboxes.append(bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use your endpoint to try to detect objects within this image. Since the image is jpeg, you use the appropriate content_type to run the prediction job. The endpoint returns a JSON file that you can simply load and peek into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in file_names:\n",
    "    with open(file_name, 'rb') as image:\n",
    "        f = image.read()\n",
    "        b = bytearray(f)\n",
    "        results = object_detector.predict(b)\n",
    "        detections = json.loads(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are in a format that is similar to the .lst format with an addition of a confidence score for each detected object. The format of the output can be represented as `[class_index, confidence_score, xmin, ymin, xmax, ymax]`. Typically, you don't consider low-confidence predictions.\n",
    "\n",
    "You have provided additional script to easily visualize the detection outputs. You can visualize the high-confidence predictions with bounding box by filtering out low-confidence detections using the script below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_categories = ['bird']\n",
    "\n",
    "# Setting a threshold 0.20 will only plot \n",
    "# detection results that have a confidence score greater than 0.20.\n",
    "# adjust this value until you only see a few colored bounding boxes\n",
    "threshold = 0.55\n",
    "\n",
    "# Visualize the detections\n",
    "max_to_display = 10\n",
    "for file_name, bboxes in zip(file_names,gt_bboxes):\n",
    "    visualize_detection(file_name, detections['prediction'], object_categories, threshold, bboxes)\n",
    "    max_to_display -= 1\n",
    "    if max_to_display == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No suprise, a model trained on 10 images is not doing so great compared to the human labels. Also, for the sake of this quick training, you trained the model with only one epoch. To achieve better detection results, you could try to tune the hyperparameters and train the model for more epochs with a larger dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the Endpoint\n",
    "\n",
    "Having an endpoint running will incur some costs. Therefore as a clean-up job, you should delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(object_detector.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
